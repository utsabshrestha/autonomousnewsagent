import datetime
import json
from random import random
import stat
from time import time, sleep
from typing import List
from DuckDuckGoSearchApi import search_duckduckgo
from GraphStates import AgentState
from GraphStates import SearchQuery
from GraphStates import SearchResult
from GraphStates import ScrapedContent
from GeminiApi import GeminiApi
from langchain_core.messages import SystemMessage, HumanMessage
from BraveSearchApi import GetBraveSearchResults, search_brave_news
import uuid
from JinaAiScrape import JinaAiScrape
from langgraph.types import Send

gemini = GeminiApi().GetInstance()

# PLANNER NODE: The list of search queries generated by Gemini
def PlannerNode(state: AgentState) -> dict :
    
    topic = state["topic"]
    today = datetime.datetime.now().strftime("%B %d, %Y")
    year = datetime.datetime.now().year
    loop_step = state.get('loop_step',0)

    discarded_queries = state["discarded_queries"]
    discardedQuries = ""
    if discarded_queries:
        discardedQuries = ", ".join(discarded_queries)

    # check if we are re-planning search queries
    if loop_step > 0:
        instruction = f"You previously generated queries that found no valid news. Try DIFFERENT keywords or specific sub-topics. Here are the previous queries you have made: [{discardedQuries}]"
    else:
        instruction = "Your goal is to Break down the topic into 3 specific web search queries."


     # 3. The System Prompt (The Rules)
    system_prompt = f"""You are a Senior News Editor. 
    {instruction}
    
    Guidelines:
    1. Focus on the most recent events ({year - 1}-{year}).
    2. Create queries that will find factual news articles, not opinion blogs.
    3. Vary the queries to cover different angles (e.g., financial impact, political response, key events).
    
    IMPORTANT: You must return ONLY a raw JSON list of strings. Do not add Markdown formatting.
    Example output: ["query 1", "query 2", "query 3", "query 4"]
    """

    # 4. The User Prompt (The Task)
    user_message = f"User Topic: {topic}\nCurrent Date: {today}\n\nGenerate search queries:"


    # 5. Call Gemini
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ]
    
    response = gemini.invoke(messages)
    
    # 6. Parse the Output (The "Array" Trick)
    # Gemini might return ```json ["query"] ```. We need to clean that.
    content = response.content.strip()
    
    # Clean up markdown code blocks if Gemini adds them
    if content.startswith("```json"):
        content = content.replace("```json", "").replace("```", "")
    elif content.startswith("```"):
        content = content.replace("```", "")

    try:
        # Convert string to Python List
        queries = json.loads(content)
    except json.JSONDecodeError:
        # Fallback if Gemini fails (rare, but good practice)
        queries = [topic] 

    searchQueries: List[SearchQuery] = []
    for query in queries:
        new_query: SearchQuery = {
            "query": query,
            "id": uuid.uuid4().hex[:8]
        }
        searchQueries.append(new_query)

    #7. Update the State
    return {"search_queries": searchQueries, "loop_step": loop_step + 1}

def SearchNodeSynchrous(state : AgentState) -> dict:
    searchResults: List[SearchResult] = []

    for query in state["search_queries"]:
        sleep(1.5)
        result = GetBraveSearchResults(search_brave_news(query["query"], 6), query)
        searchResults.extend(result)

    return {"search_results": searchResults}

def SearchNode(payload) -> dict:
    try:
        query_obj = payload["query_data"]
        target_engine = payload["engine"]
        query = payload["query_data"]["query"]
        if target_engine == "brave":
            try:
                sleep(1.5)  # Rate limit delay for Brave API
                searchResults = GetBraveSearchResults(search_brave_news(query, 6), payload["query_data"])
            except Exception as e:
                print(f"Brave searching failed {e}, falling back to DuckDuckGo")
                target_engine = "duckduckgo"

        if target_engine == "duckduckgo":
            searchResults = search_duckduckgo(query_obj, 6)
        

        return {"search_results": searchResults}
    except Exception as e:
        print(f"Exception at SearchNode {e}")
        return {"search_results": []}


# def Route_to_SearchNode(state: AgentState) -> List[Send]:
#     print(f"Generating paths for: {state['search_queries']}") 

#     return [Send("SearchNode", searchQueries) for searchQueries in state["search_queries"]]


def Route_to_SearchNode(state: AgentState) -> List[Send]:
    queries = state["search_queries"]
    tasks = []
    try:

        for i, query_obj in enumerate(queries):
            if i < 2:
                engine = "brave"
            else:
                engine = "brave"
            payload = {
                "query_data": query_obj,
                "engine": engine
            }

            tasks.append(Send("SearchNode", payload))
    except Exception as e:
        print(f"Error {e}")
    return tasks

def RemoveDuplicateSearchResults(state: AgentState) -> dict:
    all_results = state["search_results"]
    
    # Get URLs we have ALREADY successfully selected in previous loops
    # (So we don't re-evaluate URLs we already decided were good)
    previously_selected_urls = {u["url"] for u in state.get("selected_urls", [])}
    discarded_urls= set(state.get("discarded_urls", []))

    unique_candidates = []
    seen_urls = set()
    
    for result in all_results:
        url = result["url"]
        # Logic: 
        # 1. Must be unique in this batch
        # 2. Must NOT have been already selected/scraped (waste of tokens to re-check)
        # 3. If the Url is already discarded while evaluation then skip it.
        if url not in seen_urls and url not in previously_selected_urls and url not in discarded_urls:
            seen_urls.add(url)
            unique_candidates.append(result)
            
    # We overwrite 'refined_search_results' so the Evaluator only checks NEW stuff
    return {"refined_search_results": unique_candidates}

    
def SearchResultsEvaluationNode(state: AgentState) -> dict:
    topic = state["topic"]
    search_result_dict = []
    year = datetime.datetime.now().year

    for i, searchResult in enumerate(state["refined_search_results"]):
        search_result_dict.append({
                                    "id": searchResult["id"],
                                    "url": searchResult["url"],
                                   "description": searchResult["description"],
                                   "title": searchResult["title"],
                                   "page_age": searchResult["page_age"],
                                   })
    
    system_prompt = f"""You are a News Filter. 
    You are given a list of search results json with URL, Description and ID. 
    Your task is to select ONLY the results that are:
    1. Highly relevant to the User Topic.
    2. From reputable news sources (avoid ads, login pages, or unrelated blogs).
    3. Recent ({year-1}-{year} context).

    Return the output as a JSON object with a single key "relevant_id" containing the list of Id as string of the good results.
    Id is given in the search results json.
    Example: {{"relevant_id": ["a4w43", "w439sd", "349sdfs"]}}
    """

    user_message = f"User Topic: {topic}\n\nSearch Results to Filter:\n{json.dumps(search_result_dict, indent=2)}"

    messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
    response = gemini.invoke(messages)

    # 3. Parse JSON
    try:
        content = response.content.strip().replace("```json", "").replace("```", "")
        data = json.loads(content)
        valid_ids = data.get("relevant_id", [])

        good_results: List[SearchResult] = []
        bad_results= []
        discarded_queries = set()

        for result in state["refined_search_results"]:
            if result["id"] in valid_ids:
                good_results.append(result)
            else:
                bad_results.append(result["url"])
                if result["search_query"]["query"] not in discarded_queries:
                    discarded_queries.add(result["search_query"]["query"])

    
    except Exception as e:
        print(f"Error parsing filter: {e}")
        good_results = [] # Fail safe
        bad_results = []
        discarded_queries = set()

    return {"selected_urls": good_results, "discarded_urls": bad_results, "discarded_queries": list(discarded_queries)}

def ScrapeNode(state: SearchResult) -> dict:
    url = state["url"]
    scrapedContent = JinaAiScrape({"url": url})
    if scrapedContent["article_content"]:
        content: ScrapedContent = {
            "url": url,
            "content": scrapedContent["article_content"],
            "source": state,
            "image_url": scrapedContent["image_url"]
        }
        scrapedContentList : List[ScrapedContent] = []
        scrapedContentList.append(content)
        return {"scraped_contents": scrapedContentList}
    return {"scraped_contents": []}

def Route_to_ScrapeNode(state: AgentState) -> List[Send]:
    return [Send("ScrapeNode", searchResult) for searchResult in state["selected_urls"]]

def Check_Sufficient_Urls(state: AgentState):
    loop_count = state.get("loop_step", 0)
    good_urls = state.get('selected_urls', [])
    
    # Logic:
    # 1. If we have looped too many times, forced stop -> Scrape whatever we have.
    if loop_count >= 3:
        return "ScrapeNode"

    # 2. If we have enough good URLs -> Proceed to Scrape.
    if len(good_urls) >= 6:
        return "ScrapeNode"
    
    # 3. Otherwise -> Go back to Planner to find more.
    return "planner"

def ReporterNode(state: AgentState) -> dict:
    topic = state.get('topic')

    scraped_data = state["scraped_contents"]

    context_str=""

    for i, item in enumerate(scraped_data):
        source_label = i + 1
         # specific source info to help citation
        source_header = f"SOURCE_ID: [{source_label}]\nURL: {item['url']}\nTITLE: {item['source']['title']}\n"
        content = f"CONTENT:\n{item['content']}\n\n"
        context_str += source_header + content

     # 2. The System Prompt (The Rules)
    system_prompt = """You are a Senior Investigative Journalist. 
    You are given a set of raw news articles (Sources). 
    Your goal is to write a comprehensive 'Global News Briefing' on the user's topic.

    CRITICAL INSTRUCTIONS:
    1. **Synthesis**: Do not just list summaries. Combine facts from multiple sources to build a narrative.
    2. **Citations**: You must use **Standard Markdown Footnotes** for citations. In the text, use this format: "Quantum computers are fast[^1]." 
       (Notice the caret ^ inside the brackets).
       - If there are multiple sources, use: "Scientists agree[^2][^3]."
       - Every factual claim must be backed by a citation.
       - Example: "NVIDIA shares dropped 5% [^1], despite record earnings [^2]."
    3. **Verification**: If sources disagree (e.g., Source [^1] says 'Safe' and Source [^2] says 'Dangerous'), you must mention the conflict explicitly.
    4. **Structure**: 
       - Use a Main Headline.
       - Use an Executive Summary (bullet points).
       - Use Detailed Sections for deep analysis.
       - End with a "Sources" list.
    5. At the very bottom of the report, list the sources using the footnote definition format:
       [^1]: Source Name, "Article Title", Date. URL
       [^2]: Source Name, "Article Title", Date. URL
    """

    user_message = f"User Topic: {topic}\n\nRAW DATA:\n{context_str}"

    messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
    
    # We use a higher temperature (0.4) for a bit of writing flair, 
    # but low enough to keep facts straight.
    response = gemini.invoke(messages)
    
    final_report = response.content
    
    return {"final_report": final_report, "scraped_contents": scraped_data}
        
