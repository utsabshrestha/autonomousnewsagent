import datetime
import json
import stat
from typing import List
from GraphStates import AgentState
from GraphStates import SearchQuery
from GraphStates import SearchResult
from GraphStates import ScrapedContent
from GeminiApi import GeminiApi
from langchain_core.messages import SystemMessage, HumanMessage
from BraveSearchApi import search_brave_news
import uuid
from JinaAiScrape import JinaAiScrape
from langgraph.types import Send

gemini = GeminiApi().GetInstance()

# PLANNER NODE: The list of search queries generated by Gemini
def PlannerNode(state: AgentState) -> dict :
    
    topic = state["topic"]
    today = datetime.datetime.now().strftime("%B %d, %Y")
    year = datetime.datetime.now().year
    loop_step = state.get('loop_step',0)

    # check if we are re-planning search queries
    if loop_step > 0:
        instruction = "You previously generated queries that found no valid news. Try DIFFERENT keywords or specific sub-topics."
    else:
        instruction = "Your goal is to Break down the topic into 3 specific web search queries."

     # 3. The System Prompt (The Rules)
    system_prompt = f"""You are a Senior News Editor. 
    {instruction}
    
    Guidelines:
    1. Focus on the most recent events ({year - 1}-{year}).
    2. Create queries that will find factual news articles, not opinion blogs.
    3. Vary the queries to cover different angles (e.g., financial impact, political response, key events).
    
    IMPORTANT: You must return ONLY a raw JSON list of strings. Do not add Markdown formatting.
    Example output: ["query 1", "query 2", "query 3"]
    """

    # 4. The User Prompt (The Task)
    user_message = f"User Topic: {topic}\nCurrent Date: {today}\n\nGenerate search queries:"


    # 5. Call Gemini
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ]
    
    response = gemini.invoke(messages)
    
    # 6. Parse the Output (The "Array" Trick)
    # Gemini might return ```json ["query"] ```. We need to clean that.
    content = response.content.strip()
    
    # Clean up markdown code blocks if Gemini adds them
    if content.startswith("```json"):
        content = content.replace("```json", "").replace("```", "")
    elif content.startswith("```"):
        content = content.replace("```", "")

    try:
        # Convert string to Python List
        queries = json.loads(content)
    except json.JSONDecodeError:
        # Fallback if Gemini fails (rare, but good practice)
        queries = [topic] 

    searchQueries: List[SearchQuery] = []
    for query in queries:
        new_query: SearchQuery = {
            "query": query,
            "id": uuid.uuid4().hex[:8]
        }
        searchQueries.append(new_query)

    # 7. Update the State
    # return {"search_queries": searchQueries}
    return {"search_queries": [{
            "query": "Nepal politics 2025",
            "id": uuid.uuid4().hex[:8]
        }, {
            "query": "Iceland visit 2025",
            "id": uuid.uuid4().hex[:8]
        }, {
            "query": "Open Ai stock",
            "id": uuid.uuid4().hex[:8]
        }]}



def SearchNode(state: SearchQuery) -> dict:
    results = search_brave_news(state['query'], 6)
    searchResults: List[SearchResult] = []
    if results :
        for search in results:
            new_result: SearchResult = {
                    "id": uuid.uuid4().hex[:8],
                    "url": search["url"],
                    "title": search["title"],
                    "description": search["description"],
                    "page_age": search["page_age"],
                    "search_query": state
                }
            searchResults.append(new_result)

    return {"search_results": searchResults}


def Route_to_SearchNode(state: AgentState) -> List[Send]:
    return [Send("SearchNode", searchQueries) for searchQueries in state["search_queries"]]

def RemoveDuplicateSearchResults(state: AgentState) -> dict:
    all_results = state["search_results"]
    
    # Get URLs we have ALREADY successfully selected in previous loops
    # (So we don't re-evaluate URLs we already decided were good)
    previously_selected_urls = {u["url"] for u in state.get("selected_urls", [])}
    discarded_urls= set(state.get("discarded_urls", []))

    unique_candidates = []
    seen_urls = set()
    
    for result in all_results:
        url = result["url"]
        # Logic: 
        # 1. Must be unique in this batch
        # 2. Must NOT have been already selected/scraped (waste of tokens to re-check)
        # 3. If the Url is already discarded while evaluation then skip it.
        if url not in seen_urls and url not in previously_selected_urls and url not in discarded_urls:
            seen_urls.add(url)
            unique_candidates.append(result)
            
    # We overwrite 'refined_search_results' so the Evaluator only checks NEW stuff
    return {"refined_search_results": unique_candidates}

    
def SearchResultsEvaluationNode(state: AgentState) -> dict:
    topic = state["topic"]
    search_result_dict = []
    year = datetime.datetime.now().year

    for i, searchResult in enumerate(state["refined_search_results"]):
        search_result_dict.append({
                                    "id": searchResult["id"],
                                    "url": searchResult["url"],
                                   "description": searchResult["description"],
                                   "title": searchResult["title"],
                                   "page_age": searchResult["page_age"],
                                   })
    
    system_prompt = f"""You are a News Filter. 
    You are given a list of search results json with URL, Description and ID. 
    Your task is to select ONLY the results that are:
    1. Highly relevant to the User Topic.
    2. From reputable news sources (avoid ads, login pages, or unrelated blogs).
    3. Recent ({year-1}-{year} context).

    Return the output as a JSON object with a single key "relevant_id" containing the list of Id as string of the good results.
    Id is given in the search results json.
    Example: {{"relevant_id": ["a4w43", "w439sd", "349sdfs"]}}
    """

    user_message = f"User Topic: {topic}\n\nSearch Results to Filter:\n{json.dumps(search_result_dict, indent=2)}"

    messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
    response = gemini.invoke(messages)

    # 3. Parse JSON
    try:
        content = response.content.strip().replace("```json", "").replace("```", "")
        data = json.loads(content)
        valid_ids = data.get("relevant_id", [])

        good_results: List[SearchResult] = []
        bad_results= []

        for result in state["refined_search_results"]:
            if result["id"] in valid_ids:
                good_results.append(result)
            else:
                bad_results.append(result["url"])
    except Exception as e:
        print(f"Error parsing filter: {e}")
        good_results = [] # Fail safe
        bad_results = []

    return {"selected_urls": good_results, "discarded_urls": bad_results}

def ScrapeNode(state: SearchResult) -> dict:
    url = state["url"]
    scrapedContent = JinaAiScrape({"url": url})
    if scrapedContent["article_content"]:
        content: ScrapedContent = {
            "url": url,
            "content": scrapedContent["article_content"],
            "source": state,
            "image_url": scrapedContent["image_url"]
        }
        return {"scraped_contents": [content]}
    return {"scraped_contents": []}

def Route_to_ScrapeNode(state: AgentState) -> List[Send]:
    return [Send("ScrapeNode", searchResult) for searchResult in state["selected_urls"]]

def Check_Sufficient_Urls(state: AgentState):
    loop_count = state.get("loop_step", 0)
    good_urls = state.get('selected_urls', [])
    
    # Logic:
    # 1. If we have looped too many times, forced stop -> Scrape whatever we have.
    if loop_count >= 3:
        return "ScrapeNode"

    # 2. If we have enough good URLs -> Proceed to Scrape.
    if len(good_urls) >= 3:
        return "ScrapeNode"
    
    # 3. Otherwise -> Go back to Planner to find more.
    return "planner"

def ReporterNode(state: AgentState) -> dict:
    topic = state.get('topic')

    scraped_data = state["scraped_contents"]

    context_str=""

    for i, item in enumerate(scraped_data):
        source_label = i + 1
         # specific source info to help citation
        source_header = f"SOURCE_ID: [{source_label}]\nURL: {item['url']}\nTITLE: {item['source']['title']}\n"
        content = f"CONTENT:\n{item['content']}\n\n"
        context_str += source_header + content

     # 2. The System Prompt (The Rules)
    system_prompt = """You are a Senior Investigative Journalist. 
    You are given a set of raw news articles (Sources). 
    Your goal is to write a comprehensive 'Global News Briefing' on the user's topic.

    CRITICAL INSTRUCTIONS:
    1. **Synthesis**: Do not just list summaries. Combine facts from multiple sources to build a narrative.
    2. **Citations**: You MUST cite your sources using bracketed IDs like [1], [2]. 
       - Every factual claim must be backed by a citation.
       - Example: "NVIDIA shares dropped 5% [1], despite record earnings [2]."
    3. **Verification**: If sources disagree (e.g., Source [1] says 'Safe' and Source [2] says 'Dangerous'), you must mention the conflict explicitly.
    4. **Structure**: 
       - Use a Main Headline.
       - Use an Executive Summary (bullet points).
       - Use Detailed Sections for deep analysis.
       - End with a "Sources" list.
    """

    user_message = f"User Topic: {topic}\n\nRAW DATA:\n{context_str}"

    messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
    
    # We use a higher temperature (0.4) for a bit of writing flair, 
    # but low enough to keep facts straight.
    response = gemini.invoke(messages)
    
    final_report = response.content
    
    return {"final_report": final_report, "scraped_contents": scraped_data}
        
